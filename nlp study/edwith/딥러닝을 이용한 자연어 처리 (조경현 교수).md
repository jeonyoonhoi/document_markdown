# 딥러닝을 이용한 자연어 처리 (조경현 교수)





**학습내용**

- 텍스트 분류(Text Classification):

- - 문장, 문단 또는 글을 어떤 카테고리에 분류하는 작업을 텍스트 분류라고 합니다.
  - 텍스트 분류는 지도학습입니다.
  - Input: 하나의 문장, 문단 혹은 문서
  - Output: 유한한 C 개의 카테고리

> email spam / ham 

 

- 예시

- - 감성 분석
  - 카테고리 분류
  - 의도 분류

## How to represent sentence & token?

> 언어는 굉장히 임의적인 특성을 지닌다. arbitrary assignment 
>
> sentence 라는 것은 다양한 길이의 토큰이다. 
>
> 각 토큰은 미리 predefined된 vocabulary 중 하나. 





- 문장은 일련의 토큰(tokens)으로 구성되어 있습니다. 텍스트 토큰은 주관적, 임의적(arbitrary)인 성격을 띄고 있습니다.  

- 토큰을 나누는 기준은 다양합니다.

- - 공백(White space)
  - 형태소(Morphs)
  - 어절
  - 비트숫자

>  각 토큰을 vocabulary 에서 찾아서 몇번째에 있는지
>
> integer sequence로 바꿔주는것.
>
> sentence = sequence of integer
>
> 사실 인덱스도 언어처럼 arbitrary 하다. 
>
> 인코딩할 때 그 arbitrary 함을 담고 싶다. 
>
> one-hot encoding이라고도 하고, 
>
> 이 장점은 어떤 두 개의 토큰을 뽑아서 봐도 그 거리가 같다. 
>
> token assignment 가 aribitrary 하다!



- 컴퓨터에게 단어를 숫자로 표현하기 위해서, 단어장(Vocabulary)을 만들고, 중복되지 않는 인덱스(index) 로 바꿉니다.

- 궁극적으로 모든 문장을 일련의 정수로 바꿔줍니다. 이를 인코딩(Encoding)이라고 합니다.

- 하지만 관계없는 숫자의 나열로 인코딩하는 것은 우리가 원하는 것이 아닙니다. 여전히 주관적인 숫자들 뿐입니다.

- 우리는 비슷한 의미의 단어는 같이 있고, 아니면 멀리 떨어져 있는 관계를 만들고 싶습니다. 그렇다면 어떻게 관계를 만들어 줘야 할까요?

- - 한 가지 방법으로 "One hot Encoding"이 있을 수 있습니다.
    - 길이가 단어장의 총 길이(∣*V*∣)인 벡터에서, 단어의 index 위치에 있는 값은 1, 나머지는 0으로 구성합니다.
    -  *x*=[0,0,0,⋯,0,1,0,⋯,0,0]∈{0,1}∣*V*∣ 
    - 단점: 모든 토큰 간에 거리가 같습니다. 하지만 모든 단어의 뜻이 같지 않기 때문에 거리가 달라져야 저희가 원하는 단어간의 관계가 성립 됩니다. 

> 단어나 인덱스가 arbitrary 했지만, 우리가 원하는 것은 뉴럴넷이 의미를 잡아낼 수 있도록 하는 것이고, 그것을 어떻게 잡아낼 것이냐 ? continuous 한 vector space를 잡아내는 것이다. 비슷한 것과 다른 것 ==> continuous space 로 

- 어떻게 신경망이 토큰의 의미를 잡아낼수 있을까요?

- - 결론은 각 토큰을 연속 벡터 공간(Continuous vector space) 에 투영하는 방법입니다. 이를 임베딩(Embedding) 이라고도 합니다.
  - Table Look Up: 각 one hot encoding 된 토큰에게 벡터를 부여하는 과정입니다. 실질적으로 one hot encoding 벡터( *x* )와 연속 벡터 공간( *W* )을 내적 한 것 입니다.
    - 이것도 하나의 노드로 구현이 된다. 
  - Table Look Up 과정을 거친후 모든 문장 토큰은 연속적이고 높은 차원의 벡터로 변합니다.
  -  *X*=(*e*1,*e*2,⋯,*e**T*)*w**h**e**r**e* *e**t*∈*R**d* 

> fix size representation을 찾는 것 
>
>  토큰에 대한 의미를 가진 벡터를 찾는 것이 table look up 
>
> 문장에 대한 벡터를 찾는 것이 중요한 문제!



## CBow & RN & CNN

- 문장표현(Sentence representation)의 의미: 어떤 과제를 풀기에 적합하고 숫자로 나타낸 문장의 표현입니다

- CBoW(Continuous bag-of-words):

- - 단어장을 단어 주머니로 보게되고, 이에 따라 단어의 순서는 무시합니다. 
  - 문장에 대한 표현은 단어 벡터들을 평균시킨 벡터로 구합니다.
  - 효과가 좋기 때문에 제일 먼저 시도해봐야합니다. (Baseline 모델)
  - 공간상에서 가까우면 비슷한 의미, 아니면 멀리 떨어져 있을 것입니다.

> 토큰 순서 상관 없이 그냥 bag 으로 보는 것이다. 토큰들의 average를 보겠다. 
>
> continuous bag of N-gram  으로 일반화 가능 
>
> 단어를 두개, 세개를 묶어서 보는 것
>
> 장점 : order를 ignore 했지만 너무 잘된다!!
>
> sentence representation이라는게 universial 하다기 보다는 어떤 문제를 풀때 내가 푸는문제에 대해 가장 적합한 것이 무엇인가? 를 찾는 것. with this DAG, you use automatic backpropagation and stochasitic gradient desceent to train the classifier. 
>
> 어떤 문제를 풀더라도 장점은 똑 같은 method 를 적용할 수 있다는 것. 

- Relation Network(Skip-Bigram):

- - 문장안에 있는 모든 토큰 쌍(pairs)을 보고, 각 쌍에 대해서 신경망을 만들어서 문장표현을 찾습니다.
  - 장점: 여러 단어로 된 표현을 탐지 할 수 있습니다.
  - 단점: 모든 단어간의 관계를 보기 때문에, 전혀 연관이 없는 단어도 보게 됩니다.

> bigram 이라는 것은 단어나 토큰이 나란이 있는 것. 
>
> n gram n 개가 나란히 있는 것
>
> 토큰 2개를 볼건데 skip해서, 중간을 띄어서 본당. 
>
> 문장이 주어졌을 때 모든 토큰 페어를 생각해보자. 
>
> 각 페어에 대해서 뉴럴넷을 만들어서 continuous vector representation을 만들고 그것들을 합친다. 
>
> cbow 는 order를 무시한다. 
>
> 의문 => 꼭 모든 페어를 봐야 하나? => CNN

- Convolution Neural Network(CNN):
  - 특징:
    - k-gram을 계층적으로(hierachically) 보게 됩니다.
    - Layer 를 쌓을 때 마다, 점진 적으로 넓은 범위를 보기 때문에, "단어> 다중 단어 표현> 구절 > 문장"순으로 보는 인간의 인식과도 알맞습니다. 
    - 1차원의 Convolutional Network 입니다.
  - 장점: 좁은 지역간 단어의 관계를 볼수 있습니다.

> local, 1 D convolution layer를 한다.. .
>
> 1D 에서는 Kgram 이 어떤 structure를 갖고있냐를 hierarcal 하게 본다. 
>
> gradually grow!
>
> 위치마다 가까운 것을 묶어서



## Self Attention & RNN

- 지난 시간이 이야기한 CNN 과 RN 의 관계를 살펴보면 아래와 같습니다.

- RN:

  - 전체를 본다.
  - 모든 다른 토큰의 관계를 봅니다. 모든 단어간의 관계를 봐서 효율적이지 못합니다.
  -  *h**t*=*f*(*x**t*,*x*1)+⋯+*f*(*x**t*,*x**t*−1)+*f*(*x**t*,*x**t*+1)+⋯+*f*(*x**t*,*x**T*) 

- CNN:

  - 근처를 본다(너무 local )
  - 작은 범위의 토큰의 관계를 봅니다. 따라서 더 먼 거리의 단어간의 관계가 있을 경우 탐지할 수 없거나 더 많은 convolution 층을 쌓아야합니다.
  -   *h**t*=*f*(*x**t*,*x**t*−*k*)+⋯+*f*(*x**t*,*x**t*)+⋯+*f*(*x**t*,*x**t*+*k*) 

- 하지만 CNN 방식을 가중치가 부여된 RN의 일종으로 볼 수도 있습니다.

  - 가까운 애들의 weight가 높고, 먼 쪽이 weight가 작아서 무시됨

  - *h**t*=∑*t*′=1*T**I*(∣*t*′−*t*∣≤*k*)*f*(*x**t*,*x**t*′)   where  *I*(*S*)=1  if  *S*  is  *T**r**u**e* &  0  otherwise

    

- 그렇다면 가중치가 0 과 1 이 아닌 그 사이의 값으로 계산 할 수 있다면 어떨까요?

  - 만약 뉴럴넷이 계산 할 수 있다면, 거리가 아무리 멀어도 중요하면 사용하고, 그렇지 않으면 무시할 수 있지 않을까?

- Self Attention

  -  *h**t*=∑*t*′=1*T**α*(*x**t*,*x**t*′)*f*(*x**t*,*x**t*′) 
  - *α*(*x**t*,*x**t*′)=∑*t*′=1*T*exp(*β*(*x**t*,*x**t*′))exp(*β*(*x**t*,*x**t*′))
    - where  *β*(*x**t*,*x**t*′)=*R**N*(*x**t*,*x**t*′) 

  - 장점:
    - Long range & short range dependency 극복할 수 있습니다.
    - 관계가 낮은 토큰은 억제하고 관계가 높은 토큰은 강조할 수 있습니다.

  - 단점
    - 계산 복잡도가 높고 counting 같은 특정 연산이 쉽지 않습니다. 

- Recurrent Neural Network(RNN): 

  - 메모리를 가지고 있어서 현재까지 읽는 정보를 저장할 수 있습니다.

- - 문장의 정보를 시간의 순서에 따라 압축 할 수 있습니다.
  - 단점:
    - 구조 자체가 sequential 하기 떄문에, mordern hard ware와 안맞는다. 요즘에는 동시에 쫙 해야대는데 이건 하나하나 차례대로 해야돼서 느리다. 
    - 문장이 많이 길어질 수록 고정된 메모리에 압축된 정보를 담아야 하기 때문에, 앞에서 학습한 정보를 잊습니다. 이는 곧 정보의 손실을 뜻합니다. 
    - 토큰을 순차적으로 하나씩 읽어야 하기 때문에, 훈련 할때 속도가 기타 네트워크 보다 느립니다.
  - Long Term Dependency 해결방법:
    - bidirectional network를 쓰게됩니다.
    - LSTM, GRU 등 RNN의 변형을 사용합니다.



1. Cbow
2. RN(skip-bigram)
3. CNN 
4. Self-attention
5. RNN

classification하기 위해서라면 averaging등을 하게 된다. 

- Token representation : W2V, Fasttext,,

- Sentence representation

이것들을 기반으로 ! 

