# Fast Text

- 단어를 벡터로 만드는 또 다른 방법
- 페이스북에서 개발
- 워드 투 벡터 이후에 나왔음. w2v의 확장
- 가장 큰 차이점 : 패스트 텍스트는 하나의 단어 안에도 여러 단어들이 존재하는 것으로 간주, 내부단어(subword)를 고려하고 학습한다. 

### 1. 모르는 단어에 대해서 대응이 가능한 경우가 생긴다. 

패스트텍스트에서 각 단어는 글자들의 n-gram으로 나타냅니다.  n을 몇으로 결정하는지에 따라서 단어들이 얼마나 분리되는지 결정됩니다. 예를 들어 n을 3으로 잡은 트라이그램의 경우 apple은 app ppl ple 로 분리하고 이들 또한 임베딩을 합니다. 

패스트 텍스트의 뉴럴 네트워크를 학습한 후에는, 데이터 셋의 모든 단어의 각 n-gram 에 대해서 단어 임베딩이 수행됩니다. 이렇게 되면 장점은, 희귀한 단어(데이터셋에 존재하지 않음)에 대해서도 유사도를 계싼할 수 있는 경우가 생긴다는 것.

가령 패스트텍스트에서 builder 란 단어가 임베딩이 되었다면 build나 builds와 같은 단어는 builder와 n-gram에서 겹치기 때문에 해당 단어들이 단어 집합에 없었더라도 대응할 수 있게 됩니다. 



### 2. 단어 집합 내 빈도 수가 적었던 단어에 대한 대응

워드 투 벡터의 경우에는 단어 집합에서 등장 빈도 수가 높으면, 비교적 정확하게 임베딩이 되지만 등장 빈도 수가 적은 단어에 대해서는 임베딩의 정확도가 높지 않다는 단점이 있다. 쉽게 비유하면 데이터의 모수가 적다보니 정확하게 임베딩이 되지 않았다고 볼 수 있다. 

하지만 패스트 텍스트는 앞서 설명한 1번과 같은 이유로 빈도수가 적은 단어라 하더라도 유사한 단어들의 임베딩을 통해 워드투벡터와 비교하여 정확도가 높은 경향이 있습니다. 

### 3. 실습



### 4. 이미 훈련된 벡터(Pre-trained vector ) 의 활용

보통 현업에서 임베딩 벡터를 처음부터 만들지 않는다. 이미 훈련된 벡터를 갖고 와서 필요한 도메인에 대해서 추가 학습을 하는 방식으로 사용합니다. 이는 워드투벡터로 학습을 하든, 패스트텍스트로 학습을 하든,글로브로 학습을 하든 마찬가지이다. 

예를 들어 의료상담을 하는 한글 챗봇을 만들어야 한다고 한다. 이때 워드투벡터를 사용ㅎ한다면, 한글 위키피디아 같은 데이터를 워드투벡터로 이미 학습시켜놓은 벡터들을 다른 곳에서 갖고 와서는 추가적으로 의료용데이터를 워드투벡터로 학습시키는 방식이 사용될 수 있습니다. 심지어 추가 훈련조차 필요가 없는 상황이라면 더할 나위가 없을 것입니다. 

페이스북의 패스트텍스트는294개 언어에 대하여 위키피디아로 학습한 이미 훈련된 벡터들을 제공합니다. 한글 또한 제공 대상!

https://fasttext.cc/docs/en/pretrained-vectors.html

### 5. 페이스북에서 제공하는 패스트 텍스트

파이썬 genism패키지에서 패스트 텍스트를 제공하는 것으로,페이스북에서 직접 제공하는 c++ 기반의 패스트텍스트를 사용하려면 별도의 다운로드 절차를 거쳐야 합니다. 다만 윈도우에서 못함



### 6. 한글에 적용 가능한 패스트 텍스트



 











