#  3-1) TextRNN

[3-1.TextRNN](https://github.com/graykode/nlp-tutorial/tree/master/3-1.TextRNN)



# 3-2) TextLSTM



# 초보자를 위한 RNNs과 LSTM 가이드

[링크](https://skymind.ai/kr/wiki/lstm)



## Long Short-Term Memory Units(LSTM)



LSTM 은 오차의 그라디언트가 시간을 거슬러서 잘 흘러갈 수 있도록 도와줍니다. backprop하는 과정에서 오차의 값이 더 잘 유지되는데, 결과적으로 1000단계가 넘게 거슬러 올라갈 수 잇습니다. 이렇게 그라디언트가 잘 흘러간다는 것은 다시 말해 RNNs 가 더 오래전 일도 잘 기억한다는 의미입니다. 



LSTM유닛은 여러 개의 게이트(gate)가 붙어있는 셀(cell) 로 이루어져 있으며 이 셀의 정보를 새로 저장/셀의 정보를 불러오기/셀의 정보를 유지하는 기능이 있습니다. (컴퓨터의 메모리 셀과 비슷합니다.) 셀은 셀에 연결된 게이트의 값을 보고 무엇을 저장할지, 언제 정보를 내보낼지, 언제 쓰고 언제 지울지를 결정합니다. 이 게이트가 열리거나(1) 닫히는(0) 디지탈이 아니라 아날로그라는 점 주의해야 한다. 즉, 각 게이트는 0~1 의 값을 가지며 게이트의 값에 비례해서 여러 가지 작동을 합니다. 

각 게이트가 갖는 값, 즉 게이트의 계수 (또는 가중치, weight) 는 은닉층의 값과 같은 원리로 학습됩니다. 즉 게이트는 언제 신호를 불러올지/내보낼지/유지할지를 학습하며 이 학습과정은 출력의 오차를 이용한 경사하강법(gradient descent)을 사용합니다. 

아래 그림은 LSTM 유닛과 게이트의 작동 방식을 시각화한 것 입니다. 

![Alt text](C:\Users\YOONHOI\Documents\document_markdown\nlp study\image\gers_lstm.png)

유닛 그림의 맨 아래부터 보면 3개의 화살표는 LSTM유닛에 입력되는 신호이다. 이신호는 현재입력신호와 과거에서 온 피드백을 합천 것이다. 바로 입력되는 것이 아니라, 3개의 게이트로 들어가고 각 게이트에서는 입력값을 어떻게 다룰 것인지를 정합니다. 

그림에서 검은색 점 : 게이트를 나타냄

- 아래에 있는 게이트(input gate) : 얼마나 입력값을 반영할지
- 중간의 게이트 : 현재 갖고 있는 값 중 얼마를 잊을지
- 위에 있는 게이트(output gate) : 현재 갖고 있는 값 중 얼마나 출력할지

-  s_c 는 현재 LSTM유닛의 셀의 상태
- g_y_in 은 셀에 입력되는 값
- 모든 게이트는 샘플마다, 즉 매 시점마다 얼마나 열고 닫을지 결정
- 그림에서 크게 써있는 문자 = 각 게이트의 값을 반영한 결과



아래 그림은 다르게 나타낸 구조

![Alt text](C:\Users\YOONHOI\Documents\document_markdown\nlp study\image\greff_lstm_diagram.png)

LSTM 중간에 있는 더하기(+) 기호

- 일반적인 RNNs 의 유닛은 곱하기로만 이루어져 있는데
- LSTM은 피드백을 더하기로 잇고 있다. 
- 따라서 sigmoid 를 곱하다 보니 생기는 그라디언트 소실 문제가 없다

- 세 개의 (입,출,망각) 게이트는 각자 다른 가중치를 가지고 있으며 그 값으로 각 게이트에  입력으로 들어오는 값을 조절합니다. 

  ex) 망각 게이트 bias = 1로 설정하는 팁 을 활용하면 성능을 증가시킬 수도 있음 

- 기억력을 오래 보존하는 것이 목적인데 왜 망각 게이트를 갖는가?

- 때론 잊는 것이 좋을 떄가 있기 때문이다. 

- 한 작업이 끝나고 다른 것을 할 때는 기억을 다 지우고 새로 시작하는 것이 더 정확할 수 있다. 

-  RNNs 와 FFNets의 아주 큰 차이
- FFNets는 입력 하나에 출력 하나 1:1 이다. 그런데 RNNs는 설정하기에 따라 1:1 1:N N:1 N:M 등 다양하게 적용할 수 있다. 

### 다양한 단위의 시계열 데이터 분석

이렇게 시계열 데이터엔 여러가지 일이 동시에 일어납니다. 음악엔 여러 리듬이 섞여있고 글에는 다양한 주제가 등장하며 주식 시장은 각종 요인에 의해 요동칩니다. 이런 여러 사건은 동시에, 다른 시간 주기로 일어나며 LSTM은 이런 여러가지 요인을 잘 잡아냅니다.



## Gated Recurrent Units(GRUs)

RNNs 유닛의 한 종류로 GRU(Gated Recurrent Units)이 있다. 

GRU는 LSTM과 유사하지만 출력 게이트를 생략한 형태이며 따라서 항상 메모리에서 결과를 출력합니다. 

