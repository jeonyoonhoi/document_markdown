# 2018 NLP TRENDS

- 네이버 챗봇 대화팀 조용래



## 1. Transformer

- 논문 attention is all you need

  transformer 는 attention만으로 sota의 정확도와 학습 속도를 달성

  attention의 강점은 병렬처리가 가능하기 떄문에 LSTM의 속도느림을 개선

- 2017년 나옴, 그 이후로 많이 쓰였다. 



## 2. Pretrained Language model 

- word vector라는 개념은 단어 각각에 넣는것
- 취약점, 문맥에서 약했음 
- mono lingual 모델 기반으로 pretrain 을 시킵니당.
- 데이터가 만으면 참 좋은데 레이블링 된 데이터의 수는 한정적
- 따라서 labeling된 소수의 데이터로 먼저 pretraining을 시키고 나중에 unlabeled 로 더 시키는 것



- Masked LN, Next Sentence 

재미있는건  빈칸이나 다음문장 만들기 데이터는 데이터 문장 만들기가 무한정으로 생성 가능해욤

문제 A를 욜심히 풀다보니까 임베딩을열심히 학습하게 되고 이게 다른 문제 b에도 도움이 되더라 라는 개념



사실 개념이  있었지만 한두단어 정도의 연속성만 풀 수있었다. 

그래서 워드벡튜만 

ㅇ근데이건 한레이어에서만 pretrain 이었다면

이제는 문장 전ㅊ로 늘려서  다이나믹임베딩을 하는 것~~~~~~~



이미지넷 같은 경우의 아주큰걸 학습시키고 나서 작은 풀에서 사용하면 잘 된당.

cnn 같은경우 이미 이미지처리에서는transformer 를 잘 사용해서 성능을 냈었는데

NLP 는 이걸 이용한게 꽤 되었음 



## 3. Unsuperrvised Machine Translation

- IDEA parallel corpus 가 필요한데 이거 없이 번역모델 학습이 가능할까요 

- 핵심은 여러 언어를 같은 latent space로 임베딩하는 것  



1. 한 영 한 번역 // 그대로 따라하게 말고// 노이즈 추가
2. 한국어 영어 번역기하고 학습하기
3. 한국어에서 임배딩한 벡터는 어느 언어에서 왔는지 구분할 수 없어야 한다. 

==> 사이클GAN의 아이디어 똑같은 아이디어를 다르게 구현한 ㅗㄴㄴ문



 기대효과 labeled data 없이도 모델을 돌릴 수 있지 않을까



## 4. Inductive Bias : 이건모냐면

- CNN을 쓸 때
  - 한 픽셀을 이해할 때 그 주변만 본다. 
  - 이미지를 인식할때는 주변만 봐도 된다는 사전정보가 있기 때문에 
- 와 같이 여러 모델들이 사전정보를 같이 제공하여 모델을 더 강화시키는 개념



- attention 을 걸어서 classificaiton
  - 추가정보는 사람의 attention 과 model의 attention과 비슷해야한다. 



- Linguistically-Informed Self-Attention for Semantic Role Labeling



- 의미적인 것은 상단레이어에서, 구조적인(좀더 단순한) 것은 하단에서





## 5. Meta-Learning

- task 1 이 data example1 이 되는 것
- task 1  = (인풋, 모델생성, 그리고 결과나오는것)



- 이 개념이 주목을 받고 있음
- 이걸 잘하면 데이터가 적어도 학습이 잘되는 모델을 만들 수 있다. 



- meta learning for low resource neural machine translation 

초기화 ㅁ델은 GD로 하는데 메타는 gradient 를 학습하는 것 

하이퍼파라미터 중 초기값만 ~?



## QA and reasoning 

- 스쿼드 데이터셋 sQuAD 에서 딥러닝 모델은 이미 human level을 달성
- 
- 





## Common Sense 

Visual Commonsense Reasoning 

1. Grounding
2. Contextualization 
3. Reasoning 





##### Take aways

데이터가 적은데 어떻게 좋은 모델을 만들까

트렌스퍼모델을 어떻게 쓸까

좀더 현실적인 문제들은 어떻게 풀까 multimodal  